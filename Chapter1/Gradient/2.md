先不考虑神经网络、手写数字这些东西。  

假设有一个函数C(v)，v是一个二维的参数向量(v1,v2)  
如何找到C函数的全局最小值？  
![](http://neuralnetworksanddeeplearning.com/images/valley.png)  

**方法一**：
C对v求导并令导数为0，通过计算得到极值点的位置。  
缺点：不是对每个函数都适用。  
本例中参数是二维向量，实际上参数可能非常多，一一求导也不现实。  
**方法二**：
利用C的积分公式：  
$$
\begin{eqnarray} 
  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
  \frac{\partial C}{\partial v_2} \Delta v_2.
\tag{7}\end{eqnarray}
$$
只要选择正确的$$\Delta v$$使得$$\Delta C$$为负，然后调整v，C就会不断变小，最后达到极小值点。  
公式（7）又可以写成向量的形式：  
$$
\begin{eqnarray} 
  \Delta C \approx \nabla C \cdot \Delta v.
\tag{9}\end{eqnarray}
$$
根据公式（9）可知，令$$\begin{eqnarray} 
  \Delta v = -\eta \nabla C,
\tag{10}\end{eqnarray}$$
其中$$\eta$$是学习率。这样得到的$$\Delta C$$必然为负，这就是我们想到的结果。根据这个规则不断更新，就能达到极小值。  

# 关于学习率$$\eta$$

公式（7)只在一个小范围为有效，所以$$\eta$$不能太大。  
$$\eta$$又不能太小，否则学习速度太慢。  

# 关于参数

以上例子是假设v是二维向量，对于多维的情况，公式仍然适用。  

# 变种

梯度下降法有很多变种，其中有一种更接近“物理球滚落的效果”。  
*作者没有说是变种的名字和过程。根据描述需要求阶导数，我所知道的就是牛顿法和拟牛顿法*。  
这个变种有很多优点，但有一个致使缺点，就是要求二阶导数，而求二阶导数耗时。  