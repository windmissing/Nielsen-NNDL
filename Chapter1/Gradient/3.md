# 用梯度下降法最小化损失函数

用梯度下降法最小化损失函数  
$$
\begin{eqnarray}  C(w,b) \equiv \frac{1}{2n} \sum_x \| y(x) - a\|^2 \nonumber\end{eqnarray}
$$  

概率梯度下降法的更新公式得：  
$$
\begin{eqnarray}
  w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\\
  b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}.
\tag{17}\end{eqnarray}
$$

# 应用梯度下降法要解决的问题

根据以上公式可知，要更新一次w,b，就要对所有样本分别进行一次计算。  
当样本数量非常大时，每一更新都非常耗时。  

解决方法：**随机梯度下降法 stochastci gradient descent**  

随机取m个样本来更新w和b，认为**m个样本计算出来的梯度近似C的梯度。**   
$$
\begin{eqnarray}
  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,
\tag{18}\end{eqnarray}
$$
因此，更新公式变成了：  
$$
\begin{eqnarray} 
  w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}\\
  b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial b_l},
\tag{21}\end{eqnarray}
$$

事实上，公式中的$$\frac{1}{n}$$或$$\frac{1}{m}$$也不重要，常常被省略。只是相当于调整了$$\eta$$而已。  

# 批量梯度下降法与随机梯度下降法的比较

随机梯度下降法不如批量梯度下降法那样“直奔目的”。它会为随机选取的样本不同而波动。
但随机梯度下降法带来的优化的效率是非常可观的。  
事实上，我们不需要非常完美的算法，只要算法的进化方向是正确的就可以了。  
所以随机梯度下降法更常用。  

更进一步，当m=1时，每次只是随机选择一个样本进行调参，就变成了一个在线学习算法。  

# 怎么思考三维以上维度的问题  

人不能看见三维以上维度的空间。所以需要借助一些方法来思考三维以上维度的问题。  
其中一种方法是algebraic(代数)。  
还有其它一些方法，见[链接](http://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking)