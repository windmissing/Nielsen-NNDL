使用类Network来代表神经网络。  
函数`def __init__(self, sizes):`为神经网络的初始化。  

# size
Network构造函数的参数size是一个list。  
list的个数代表网络的层数。  
list中的每一项代表这一层的神经元个数。  
例如：size = [2,3,1]，说明：  
输入层：2个输入神经元  
输出层：1个输出神经元  
中间层：共1层，有3个神经元  

# bias  

bias代表偏移。  初始化后的b为一个list.  
list b中有（层数-1）项。因为输入层不需要b。中间层和输出层每层对应一项。  
b中的每一项又各是一个np.array（列向量）。这些array中的元素个数各不相同，分别与该项对应的层的神经元相对应。  
例如根据以上size得：  
$$
bias = 
\begin{bmatrix}
\begin{bmatrix}
[b1_1] \\
[b1_2] \\
[b1_3]
\end{bmatrix},[[b2_1]]
\end{bmatrix}
$$
假设输入层为第0层，$$bi_j$$代表第i层上第j个神经元的b  
所有的$$bi_j$$都是均值为0方差为1的随机数  

# weights

weights代表权重w。  
每个神经元只有1个b，却有一组w，w的个数与它的参数个数有关，因为w的初始化比b要复杂的多。  
在目前学习的神经网络结构中，上一层所有神经元的输出会成为下一层所有神经元的输入。所以，某一层神经元的输入个数相同，且**这一层神经的参数个数 = 上一层的神经元个数**。  
list weights中同样有（层数-1）项。因为输入层不需要weights。中间层和输出层每层对应一项。  
weights中的每一项又各是一个二维的np.array（数组）。数组的大小为（当前层神经元个数 * 上一层神经元个数）。   
例如根据以上size得：  
$$
weights = 
\begin{bmatrix}
\begin{bmatrix}
[w1_{11}, w1_{12}] \\
[w1_{21}, w1_{22}] \\
[w1_{31}, w1_{32}
\end{bmatrix},[[w2_{11}, w2_{12}, w2_{13}]]
\end{bmatrix}
$$
假设输入层为第0层，$$bi_{jk}$$代表第i-1层上的第k个神经元的输出对第i层第j个神经元的重要性。  
所有的$$wi_{jk}$$都是均值为0方差为1的随机数  

