令  
$$\delta^l_j$$：l层第j个神经元的error。  
$$\delta^l$$：l层神经元的error向量。  

# $$\Delta z^l_j$$
已知第l层第j个神经元的输出为：  
$$
a^l_j = \sigma(z^l_j)
$$
假设在[神经元的加权输入](https://windmising.gitbook.io/nielsen-nndl/introduction-1/warmup)上加上一点扰动，使得：  
$$
a^l_j = \sigma(z^l_j+\Delta z^l_j)
$$
a^l_j的变化通过网络传播到下一层，使得最终的代价也会变化。变化量为：  
$$
\frac{\partial C}{\partial z^l_j} \Delta z^l_j
$$

# 让最终的cost尽量小

如果我们希望找到一个合适的$$\Delta z^l_j$$，使得cost变小。  
场景一：$$|\frac{\partial C}{\partial z^l_j}|$$非常大。  
只需要令$$\Delta z^l_j$$为一个与$$\frac{\partial C}{\partial z^l_j}$$相反的数，就可以有效低使cost变小。  
场景二：$$|\frac{\partial C}{\partial z^l_j}|$$接近0。  
很难通过$$\Delta z^l_j$$来改变cost。那么认为这个神经元已经在最优状态附近了，很难有改进空间。  

# $$\delta^l_j$$

通过上面的分析可以发现，$$\frac{\partial C}{\partial z^l_j}$$可以用于衡量这个神经元离最优状态还有多远。  
定义：  
$$
\begin{eqnarray} 
  \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.
\tag{29}\end{eqnarray}
$$

反向传播算法会先计算每个神经元的$$\delta^l_j$$，再根据$$\delta^l_j$$去计算它们的w和b

# 其它

问：为什么改变的是神经元的加权输入而不是神经元的输入?
答：用神经元的输入来计算会增加计算的复杂度。  