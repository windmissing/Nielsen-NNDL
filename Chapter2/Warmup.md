# 神经元视角  

## 定义

$$w_{jk}^l$$：l-1层第k个神经与l层第j个神经元的连接的权重。  
*对于这个符号，我更喜欢这么解释：l-1层第k个神经元对l层第j个神经元的重要性。*  
$$b_j^l$$：l层第j个神经元的偏移  
$$a_j^l$$：l层第j个神经元的输出  

## 公式

$$
\begin{eqnarray} 
  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),
\tag{23}\end{eqnarray}
$$

# 矩阵视角

把公式23以矩阵的形式表达出来。  

## 定义

矩阵$$w^l$$：第l层所有神经元的权重。$$w_{jk}^l$$为$$w^l$$的j行k列。  
向量$$b^l$$：第l层所有神经元的偏移。  
向量$$a^l$$：同上。  
向量函数$$\sigma(v)$$：对向量v中的每一个元素做$$\sigma$$然后把结果再合并成一个向量。即：$$\sigma(v)_j = \sigma(v_j)$$  

## 公式

$$
\begin{eqnarray} 
  a^{l} = \sigma(w^l a^{l-1}+b^l).
\tag{25}\end{eqnarray}
$$

用矩阵方式写公式的好处：  
1. 更简洁。  
2. 更少的上/下标。  
3. 向量运算更快。  

## 引申

令:  
$$
z^l \equiv w^l a^{l-1}+b^l
$$
称zl为l层神经元的加权输入。  
在后面的章节中，zl将有特殊的用处。  