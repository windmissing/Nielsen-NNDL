**好的代价函数可以让神经网络学习更快。**

# 只有一个神经元的情况

假设只有一个神经元的情况：  

![](http://neuralnetworksanddeeplearning.com/images/tikz28.png)
输入x = 1，期望输出 y = 0  
令初始参数w=0.2， b=0.2，学习率eta=0.15，迭代效果如下：  
![](http://windmissing.github.io/images_for_gitbook/Nielsen-NNDL/2.png)  
可以看出，当错误偏差大的时候学习速度很慢。
这个问题不只是出现在这个神经元中，在神经网络中也是同样的情况。    
这与人的学习方式不符，人在开始时会觉得比较快。  

# 问：为什么当错误严重时反而学习速度慢？  
答：对于一个样本来说，二次代价函数为：  
$$
\begin{eqnarray}
  C = \frac{(y-a)^2}{2},
\tag{54}\end{eqnarray}
$$
二次代价函数对任意w、b的偏导为：  
$$
\begin{eqnarray} 
  \frac{\partial C}{\partial w} & = & (a-y)\sigma'(z) x = a \sigma'(z) \tag{55}\\
  \frac{\partial C}{\partial b} & = & (a-y)\sigma'(z) = a \sigma'(z),
\tag{56}\end{eqnarray}
$$
回顾一下sigmoid函数的形状：  
![](http://windmissing.github.io/images_for_gitbook/mathematics_basic_for_ML/2.png)  
可以看出，当a接近0或1时，w和b的改变会非常小。  