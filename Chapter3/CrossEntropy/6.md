这是另一种解决learning slow down的方法。  

# 激活函数softmax

将神经网络的输出层神经元全部换成softmax神经元。  
softmax神经元的激活函数为：  
$$
\begin{eqnarray} 
  a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},
\tag{78}\end{eqnarray}
$$

softmax激活函数满足以下特点：  
1. $$\sum_j a^L_j = 1$$  
2. $$a^L_j > 0$$  

softmax层的输出是一种概率分布。  
可以看作是“这个神经元的是正确的”的概率。  

$$
z^L_j = \ln a^L_j + C
$$

# 代价函数log-likelihood

定义代价函数为：  
$$
\begin{eqnarray}
  C \equiv -\ln a^L_y.
\tag{80}\end{eqnarray}
$$

例如，如果y=7是正确结果，那么$$a^L_7$$应该接近1，而C应该接近0.  

# softmax + log-likelihood怎么解决“learn slow”的问题

通过推导可得：  
$$
\begin{eqnarray}
  \frac{\partial C}{\partial b^L_j} & = & a^L_j-y_j  \tag{81}\\
  \frac{\partial C}{\partial w^L_{jk}} & = & a^{L-1}_k (a^L_j-y_j) 
\tag{82}\end{eqnarray}
$$
这个结果与sigmoid + cross-entropy的结果是一样的。  
因此也能解决"learn slow"的问题。  

# “softmax + log-likelihood” VS “sigmoid + cross-entropy”  

在大多数情况下，这两种方法都有比较好的效果。  
softmax + log-likelihood有一个额外的好处，就是输出的是一个概率分布。  
本书主要使用“sigmoid + cross-entropy”。

