# sigmoid神经元存在的问题

已经w的偏导公式为：  
$$
\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.
\tag{BP4}\end{eqnarray}
$$
已知$$a^{l-1}_k \in [0,1]$$必定为正，$$\frac{\partial C}{\partial w^l_{jk}}$$的符号由$$\delta^l_j$$决定。  
可以看书：  
$$\delta^l_j$$的值与k无关  
$$\Rightarrow \forall k,\frac{\partial C}{\partial w^l_{jk}}$$的符号相同  
$$\Rightarrow$$对于一个神经元中所有的w，会同时变大或变小  
这是不合理的。  

# 改进的方法：tanh神经元  

tanh神经元的激活函数为：  
$$
\begin{eqnarray}
  \tanh(z) \equiv \frac{e^z-e^{-z}}{e^z+e^{-z}}.
\tag{110}\end{eqnarray}
$$

tanh其实只是对sigmoid的变形，其图形为：  
![](http://windmissing.github.io/images_for_gitbook/Nielsen-NNDL/5.png)  
tanh和sigmoid的重要区别是$$tanh(z) \in [-1,1]$$，这使的同一个神经元中不同的w的偏导的符号可以不同。  

# tanh的效果

理论上分析，tanh能弥补sigmoid的不足，效果应该优于sigmoid。  
在实践中，并没有明显的证据表明tanh优于sigmoid。
