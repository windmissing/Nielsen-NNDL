rectified linear神经元的激活函数为：  
$$
a(z) = \max(0, z)
$$

图像为：  
![](http://windmissing.github.io/images_for_gitbook/Nielsen-NNDL/6.png)  

sigmoid和tanh因为$$\sigma'(z)$$而存在“学得慢”的问题。  
rectified linear神经元可以解决这一问题。  

[?]这句话没看懂  
> On the other hand, when the weighted input to a rectified linear unit is negative, the gradient vanishes, and so the neuron stops learning entirely.

在图像识别领域有较好的效果。  