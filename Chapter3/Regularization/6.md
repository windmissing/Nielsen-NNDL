过程：  
1. 随机选择中间层中一半的神经元“删掉”。  
![](http://neuralnetworksanddeeplearning.com/images/tikz31.png)  
2. 对剩下的神经元正向传播求z和a  
3. 对剩下的神经元反向传播求w和b的偏导  
4. 更新剩下的神经元的w和b  
5. 恢复删掉的神经  
6. 重复1-5步  

[?]原文中这一段话没看懂
>When we actually run the full network that means that twice as many hidden neurons will be active. To compensate for that, we halve the weights outgoing from the hidden neurons.

# 为什么drop out能抑制过拟合？  

答1：如果同样的数据训练不同的网络，这些网络会产生不同的过拟合而得到不同的结果。  
将所以网络的结果合起来，可以消除单个网络的过拟合。  
drop out的过程相当于产生不同的网络。  

答2：某个神经不应该过于依赖另一个神经元。  
任意一个神经元的不存在，都不应该影响整体的结果。  
这样的网络更具有鲁棒性。  

drop out算法效果惊人且应用广泛。