# shared weights
一个local receptive field对应的一个下一层神经元的output计算公式为：  
$$
\begin{eqnarray} 
 a_{jk} = \sigma\left(b + \sum_{l=0}^4 \sum_{m=0}^4  w_{l,m} a_{j+l, k+m} \right).
\tag{125}\end{eqnarray}
$$

从公式可以看出，计算所使用的w与jk（哪一个local receptive field）无关，只与lm（在local receptive field中的相对位置）有关。  
这是因为所有的神经元的计算共用一组w和一个b。这就是**shared weights and biases**。  

# 意义

由一个输入层到一个卷积层的映射叫作**feature map**.  
一个feature map共享一组w和一个b。参数w和b称为**kernel**或**filter**。  
一个feature map用于在输入层所有位置寻找同样的特征。  
一个输入层可以建立多个（可能是很多个）feature map。  

# 优点  

共享变量极大地减少了参数的数量。  
以5*5的local receptive field为例，只需要26个变量。  

# “卷积”名字的由来  

公式125称为卷积公式。它还可以写成这样：  
$$
a^1 = \sigma(b + w * a^0)
$$
其中`*`称为卷积操作。  