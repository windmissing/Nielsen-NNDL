# Summary

* [Introduction](README.md)
* [第1章 使用神经网络识别手写数字](Chapter1/Introduction.md)
    * [感知机神经元](Chapter1/Perceptrons.md)
    * [sigmoid神经元](Chapter1/Sigmoid.md)
    * [神经网络的架构](Chapter1/Architecture.md)
    * [用于识别手写数字的简单网络](Chapter1/Handwritten.md)
    * 梯度下降学习法
        * [准备工作](Chapter1/Gradient/1.md)
        * [梯度下降法](Chapter1/Gradient/2.md)
        * [应用到神经网络](Chapter1/Gradient/3.md)
    * 实现数字分类的神经网络
        * [数据集](Chapter1/Implementing/1.md)
        * [初始化](Chapter1/Implementing/2.md)
        * [向后传递](Chapter1/Implementing/3.md)
        * [随机梯度下降](Chapter1/Implementing/4.md)
        * [总结](Chapter1/Implementing/5.md)
* [第2章 反向传播算法的工作原理](Chapter2/Introduction.md)
    * [热身：一种矩阵方法快速计算神经网络的输出](Chapter2/Warmup.md)
    * [关于代价函数的两个假设](Chapter2/Assumptions.md)
    * [Hadamard积](Chapter2/Hadamard.md)
    * [反向传播算法中的4个等式](Chapter2/Equations/Advices.md)
        * [一个新的定义](Chapter2/Equations/1.md)
        * [4个等式](Chapter2/Equations/2.md)
        * [等式的意义](Chapter2/Equations/3.md)
    * [4个等式的证明](Chapter2/Proof.md)
    * [反向传播算法](Chapter2/Algorithm.md)
    * [代码解读](Chapter2/Code.md)
    * [反向传播算法为什么这么快](Chapter2/Fast.md)
* [第3章 提升神经网络的学习方法](Chapter3/Introduction.md)
    * cross-entropy代价函数
        * [当前神经网络存在的问题](Chapter3/CrossEntropy/1.md)
        * [引入cross-entropy代价函数](Chapter3/CrossEntropy/2.md)
        * [使用cross-entropy分类手写数字](Chapter3/CrossEntropy/3.md)
        * [cross-entropy代价函数是怎么推出来的](Chapter3/CrossEntropy/4.md)
        * [cross-entropy的数学意义](Chapter3/CrossEntropy/5.md)
        * [softmaxt+loglikelihood](Chapter3/CrossEntropy/6.md)
    * 过拟合和正则化
        * [过拟合](Chapter3/Regularization/1.md)
        * [L2正则化](Chapter3/Regularization/2.md)
        * [在当前神经网络中使用L2正则化](Chapter3/Regularization/3.md)
        * [其它问题](Chapter3/Regularization/4.md)
        * [L1正则化](Chapter3/Regularization/5.md)
        * [dropout正则化](Chapter3/Regularization/6.md)
        * [人为扩充训练数据](Chapter3/Regularization/7.md)
    * [weights初始化](Chapter3/Weights.md)
    * [回到手势识别代码](Chapter3/Code.md)
    * 怎样选择超参数
        * [broad策略](Chapter3/HyperParameters/1.md)
        * [学习率eta](Chapter3/HyperParameters/2.md)
        * [迭代次数epochs](Chapter3/HyperParameters/3.md)
        * [正则化参数lambda](Chapter3/HyperParameters/4.md)
        * [minibatch样本数m](Chapter3/HyperParameters/5.md)
        * [自动化技术](Chapter3/HyperParameters/6.md)
    * 其它技术
        * [Hessian技术](Chapter3/Other/1.md)
        * [momentum技术](Chapter3/Other/2.md)
        * [tanh神经元](Chapter3/Other/3.md)
        * [RectifiedLinear神经元](Chapter3/Other/4.md)
* [第5章 训练深度神经网络难以训练](Chapter5/Introduction.md)
    * [梯度消失问题](Chapter5/Vanishing.md)
    * [梯度消失的原因](Chapter5/Causing.md)
* 第6章 深度学习
    * [卷积神经网络介绍](Chapter6/Convolutional/1.md)
        * [LocalReceptiveField](Chapter6/Convolutional/2.md)
        * [SharedWeights](Chapter6/Convolutional/3.md)
        * [pooling层](Chapter6/Convolutional/4.md)
        * [组装到一起](Chapter6/Convolutional/5.md)
    * [卷积神经网络的实践与改进](Chapter6/Practise.md)
    * [其它深度神经网络的方法])(Chapter6/Other.md)
* [术语中英文对照](norms.md)